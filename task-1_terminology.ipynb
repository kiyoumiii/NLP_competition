{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1 - 速通 Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-08T12:50:49.017025Z",
     "iopub.status.busy": "2024-07-08T12:50:49.016753Z",
     "iopub.status.idle": "2024-07-08T12:50:50.972508Z",
     "shell.execute_reply": "2024-07-08T12:50:50.971857Z",
     "shell.execute_reply.started": "2024-07-08T12:50:49.017005Z"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-07-13T06:26:45.987867500Z",
     "start_time": "2024-07-13T06:26:43.295025900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchtext in c:\\users\\administrator\\appdata\\roaming\\python\\python311\\site-packages (0.18.0)\n",
      "Requirement already satisfied: tqdm in e:\\anaconda\\lib\\site-packages (from torchtext) (4.65.0)\n",
      "Requirement already satisfied: requests in e:\\anaconda\\lib\\site-packages (from torchtext) (2.31.0)\n",
      "Requirement already satisfied: torch>=2.3.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python311\\site-packages (from torchtext) (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\administrator\\appdata\\roaming\\python\\python311\\site-packages (from torchtext) (1.26.4)\n",
      "Requirement already satisfied: filelock in e:\\anaconda\\lib\\site-packages (from torch>=2.3.0->torchtext) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python311\\site-packages (from torch>=2.3.0->torchtext) (4.12.2)\n",
      "Requirement already satisfied: sympy in e:\\anaconda\\lib\\site-packages (from torch>=2.3.0->torchtext) (1.11.1)\n",
      "Requirement already satisfied: networkx in e:\\anaconda\\lib\\site-packages (from torch>=2.3.0->torchtext) (3.1)\n",
      "Requirement already satisfied: jinja2 in e:\\anaconda\\lib\\site-packages (from torch>=2.3.0->torchtext) (3.1.2)\n",
      "Requirement already satisfied: fsspec in e:\\anaconda\\lib\\site-packages (from torch>=2.3.0->torchtext) (2023.4.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\administrator\\appdata\\roaming\\python\\python311\\site-packages (from torch>=2.3.0->torchtext) (2021.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anaconda\\lib\\site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda\\lib\\site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda\\lib\\site-packages (from requests->torchtext) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda\\lib\\site-packages (from requests->torchtext) (2024.7.4)\n",
      "Requirement already satisfied: colorama in e:\\anaconda\\lib\\site-packages (from tqdm->torchtext) (0.4.6)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\administrator\\appdata\\roaming\\python\\python311\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=2.3.0->torchtext) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\administrator\\appdata\\roaming\\python\\python311\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=2.3.0->torchtext) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\anaconda\\lib\\site-packages (from jinja2->torch>=2.3.0->torchtext) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in e:\\anaconda\\lib\\site-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-08T12:50:50.974203Z",
     "iopub.status.busy": "2024-07-08T12:50:50.973877Z",
     "iopub.status.idle": "2024-07-08T12:50:52.080997Z",
     "shell.execute_reply": "2024-07-08T12:50:52.080232Z",
     "shell.execute_reply.started": "2024-07-08T12:50:50.974178Z"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-07-13T06:26:45.990867200Z",
     "start_time": "2024-07-13T06:26:45.985626200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T12:50:52.082249Z",
     "iopub.status.busy": "2024-07-08T12:50:52.081911Z",
     "iopub.status.idle": "2024-07-08T12:50:52.092084Z",
     "shell.execute_reply": "2024-07-08T12:50:52.091346Z",
     "shell.execute_reply.started": "2024-07-08T12:50:52.082230Z"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-07-13T06:26:45.995432Z",
     "start_time": "2024-07-13T06:26:45.990867200Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义数据集类\n",
    "# 修改TranslationDataset类以处理术语\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, filename, terminology):\n",
    "        self.data = []\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                en, zh = line.strip().split('\\t')\n",
    "                self.data.append((en, zh))\n",
    "        \n",
    "        self.terminology = terminology\n",
    "        \n",
    "        # 创建词汇表，注意这里需要确保术语词典中的词也被包含在词汇表中\n",
    "        self.en_tokenizer = get_tokenizer('basic_english')\n",
    "        self.zh_tokenizer = list  # 使用字符级分词\n",
    "        \n",
    "        en_vocab = Counter(self.terminology.keys())  # 确保术语在词汇表中\n",
    "        zh_vocab = Counter()\n",
    "        \n",
    "        for en, zh in self.data:\n",
    "            en_vocab.update(self.en_tokenizer(en))\n",
    "            zh_vocab.update(self.zh_tokenizer(zh))\n",
    "        \n",
    "        # 添加术语到词汇表\n",
    "        self.en_vocab = ['<pad>', '<sos>', '<eos>'] + list(self.terminology.keys()) + [word for word, _ in en_vocab.most_common(10000)]\n",
    "        self.zh_vocab = ['<pad>', '<sos>', '<eos>'] + [word for word, _ in zh_vocab.most_common(10000)]\n",
    "        \n",
    "        self.en_word2idx = {word: idx for idx, word in enumerate(self.en_vocab)}\n",
    "        self.zh_word2idx = {word: idx for idx, word in enumerate(self.zh_vocab)}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        en, zh = self.data[idx]\n",
    "        en_tensor = torch.tensor([self.en_word2idx.get(word, self.en_word2idx['<sos>']) for word in self.en_tokenizer(en)] + [self.en_word2idx['<eos>']])\n",
    "        zh_tensor = torch.tensor([self.zh_word2idx.get(word, self.zh_word2idx['<sos>']) for word in self.zh_tokenizer(zh)] + [self.zh_word2idx['<eos>']])\n",
    "        return en_tensor, zh_tensor\n",
    "\n",
    "def collate_fn(batch):\n",
    "    en_batch, zh_batch = [], []\n",
    "    for en_item, zh_item in batch:\n",
    "        en_batch.append(en_item)\n",
    "        zh_batch.append(zh_item)\n",
    "    \n",
    "    # 对英文和中文序列分别进行填充\n",
    "    en_batch = nn.utils.rnn.pad_sequence(en_batch, padding_value=0, batch_first=True)\n",
    "    zh_batch = nn.utils.rnn.pad_sequence(zh_batch, padding_value=0, batch_first=True)\n",
    "    \n",
    "    return en_batch, zh_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T12:50:52.093405Z",
     "iopub.status.busy": "2024-07-08T12:50:52.093120Z",
     "iopub.status.idle": "2024-07-08T12:50:52.102203Z",
     "shell.execute_reply": "2024-07-08T12:50:52.101497Z",
     "shell.execute_reply.started": "2024-07-08T12:50:52.093388Z"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-07-13T06:26:46.001748400Z",
     "start_time": "2024-07-13T06:26:45.998432200Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: [batch_size, src_len]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded shape: [batch_size, src_len, emb_dim]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # outputs shape: [batch_size, src_len, hid_dim]\n",
    "        # hidden shape: [n_layers, batch_size, hid_dim]\n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # input shape: [batch_size, 1]\n",
    "        # hidden shape: [n_layers, batch_size, hid_dim]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded shape: [batch_size, 1, emb_dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        # output shape: [batch_size, 1, hid_dim]\n",
    "        # hidden shape: [n_layers, batch_size, hid_dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        # prediction shape: [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src shape: [batch_size, src_len]\n",
    "        # trg shape: [batch_size, trg_len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        _, hidden = self.encoder(src)\n",
    "        \n",
    "        input = trg[:, 0].unsqueeze(1)  # Start token\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T06:26:46.008813600Z",
     "start_time": "2024-07-13T06:26:46.003751700Z"
    }
   },
   "outputs": [],
   "source": [
    "# 新增术语词典加载部分\n",
    "def load_terminology_dictionary(dict_file):\n",
    "    terminology = {}\n",
    "    with open(dict_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            en_term, ch_term = line.strip().split('\\t')\n",
    "            terminology[en_term] = ch_term\n",
    "    return terminology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T12:50:52.103387Z",
     "iopub.status.busy": "2024-07-08T12:50:52.103142Z",
     "iopub.status.idle": "2024-07-08T12:50:52.107862Z",
     "shell.execute_reply": "2024-07-08T12:50:52.107237Z",
     "shell.execute_reply.started": "2024-07-08T12:50:52.103369Z"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-07-13T06:26:46.043130500Z",
     "start_time": "2024-07-13T06:26:46.010816900Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].contiguous().view(-1, output_dim)\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-08T12:50:52.108867Z",
     "iopub.status.busy": "2024-07-08T12:50:52.108637Z"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-07-13T07:27:03.284559300Z",
     "start_time": "2024-07-13T06:26:46.018131600Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 42\u001B[0m\n\u001B[0;32m     39\u001B[0m CLIP \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(N_EPOCHS):\n\u001B[1;32m---> 42\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m train(model, train_loader, optimizer, criterion, CLIP)\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m02\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Train Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     45\u001B[0m \u001B[38;5;66;03m# 在训练循环结束后保存模型\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[20], line 12\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, iterator, optimizer, criterion, clip)\u001B[0m\n\u001B[0;32m     10\u001B[0m trg \u001B[38;5;241m=\u001B[39m trg[:, \u001B[38;5;241m1\u001B[39m:]\u001B[38;5;241m.\u001B[39mcontiguous()\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     11\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output, trg)\n\u001B[1;32m---> 12\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     13\u001B[0m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(model\u001B[38;5;241m.\u001B[39mparameters(), clip)\n\u001B[0;32m     14\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    524\u001B[0m     )\n\u001B[1;32m--> 525\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[0;32m    526\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[0;32m    527\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 267\u001B[0m _engine_run_backward(\n\u001B[0;32m    268\u001B[0m     tensors,\n\u001B[0;32m    269\u001B[0m     grad_tensors_,\n\u001B[0;32m    270\u001B[0m     retain_graph,\n\u001B[0;32m    271\u001B[0m     create_graph,\n\u001B[0;32m    272\u001B[0m     inputs,\n\u001B[0;32m    273\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    274\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    275\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    745\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    746\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 主函数\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()  # 开始计时\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    #terminology = load_terminology_dictionary('../dataset/en-zh.dic')\n",
    "    terminology = load_terminology_dictionary('./dataset/en-zh.dic')\n",
    "\n",
    "    # 加载数据\n",
    "    dataset = TranslationDataset('./dataset/train.txt',terminology = terminology)\n",
    "    # 选择数据集的前N个样本进行训练\n",
    "    # N = 1000  #int(len(dataset) * 1)  # 或者你可以设置为数据集大小的一定比例，如 int(len(dataset) * 0.1)\n",
    "    # subset_indices = list(range(N))\n",
    "    # subset_dataset = Subset(dataset, subset_indices)\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # 定义模型参数\n",
    "    INPUT_DIM = len(dataset.en_vocab)\n",
    "    OUTPUT_DIM = len(dataset.zh_vocab)\n",
    "    ENC_EMB_DIM = 256\n",
    "    DEC_EMB_DIM = 256\n",
    "    HID_DIM = 512\n",
    "    N_LAYERS = 2\n",
    "    ENC_DROPOUT = 0.5\n",
    "    DEC_DROPOUT = 0.5\n",
    "\n",
    "    # 初始化模型\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "    model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "    # 定义优化器和损失函数\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=dataset.zh_word2idx['<pad>'])\n",
    "\n",
    "    # 训练模型\n",
    "    N_EPOCHS = 20\n",
    "    CLIP = 1\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "        print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f}')\n",
    "        \n",
    "    # 在训练循环结束后保存模型\n",
    "    torch.save(model.state_dict(), './translation_model_GRU.pth')\n",
    "    \n",
    "    end_time = time.time()  # 结束计时\n",
    "\n",
    "    # 计算并打印运行时间\n",
    "    elapsed_time_minute = (end_time - start_time)/60\n",
    "    print(f\"Total running time: {elapsed_time_minute:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 在开发集上进行模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-13T07:27:03.287563700Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sacrebleu.metrics import BLEU\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T07:27:03.287563700Z",
     "start_time": "2024-07-13T07:27:03.287563700Z"
    }
   },
   "outputs": [],
   "source": [
    "# 假设我们已经定义了TranslationDataset, Encoder, Decoder, Seq2Seq类\n",
    "\n",
    "def load_sentences(file_path: str) -> List[str]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f]\n",
    "\n",
    "# 更新translate_sentence函数以考虑术语词典\n",
    "def translate_sentence(sentence: str, model: Seq2Seq, dataset: TranslationDataset, terminology, device: torch.device, max_length: int = 50):\n",
    "    model.eval()\n",
    "    tokens = dataset.en_tokenizer(sentence)\n",
    "    tensor = torch.LongTensor([dataset.en_word2idx.get(token, dataset.en_word2idx['<sos>']) for token in tokens]).unsqueeze(0).to(device)  # [1, seq_len]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, hidden = model.encoder(tensor)\n",
    "\n",
    "    translated_tokens = []\n",
    "    input_token = torch.LongTensor([[dataset.zh_word2idx['<sos>']]]).to(device)  # [1, 1]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        output, hidden = model.decoder(input_token, hidden)\n",
    "        top_token = output.argmax(1)\n",
    "        translated_token = dataset.zh_vocab[top_token.item()]\n",
    "        \n",
    "        if translated_token == '<eos>':\n",
    "            break\n",
    "        \n",
    "        # 如果翻译的词在术语词典中，则使用术语词典中的词\n",
    "        if translated_token in terminology.values():\n",
    "            for en_term, ch_term in terminology.items():\n",
    "                if translated_token == ch_term:\n",
    "                    translated_token = en_term\n",
    "                    break\n",
    "        \n",
    "        translated_tokens.append(translated_token)\n",
    "        input_token = top_token.unsqueeze(1)  # [1, 1]\n",
    "\n",
    "    return ''.join(translated_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-13T07:27:03.288563800Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_bleu(model: Seq2Seq, dataset: TranslationDataset, src_file: str, ref_file: str, terminology,device: torch.device):\n",
    "    model.eval()\n",
    "    src_sentences = load_sentences(src_file)\n",
    "    ref_sentences = load_sentences(ref_file)\n",
    "    \n",
    "    translated_sentences = []\n",
    "    for src in src_sentences:\n",
    "        translated = translate_sentence(src, model, dataset, terminology, device)\n",
    "        translated_sentences.append(translated)\n",
    "    \n",
    "    bleu = BLEU()\n",
    "    score = bleu.corpus_score(translated_sentences, [ref_sentences])\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-13T07:27:03.289563400Z"
    }
   },
   "outputs": [],
   "source": [
    "# 主函数\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 加载术语词典\n",
    "    terminology = load_terminology_dictionary('../dataset/en-zh.dic')\n",
    "    \n",
    "    # 创建数据集实例时传递术语词典\n",
    "    dataset = TranslationDataset('../dataset/train.txt', terminology)\n",
    "    \n",
    "\n",
    "    # 定义模型参数\n",
    "    INPUT_DIM = len(dataset.en_vocab)\n",
    "    OUTPUT_DIM = len(dataset.zh_vocab)\n",
    "    ENC_EMB_DIM = 256\n",
    "    DEC_EMB_DIM = 256\n",
    "    HID_DIM = 512\n",
    "    N_LAYERS = 2\n",
    "    ENC_DROPOUT = 0.5\n",
    "    DEC_DROPOUT = 0.5\n",
    "\n",
    "    # 初始化模型\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "    model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "    # 加载训练好的模型\n",
    "    model.load_state_dict(torch.load('./translation_model_GRU.pth'))\n",
    "\n",
    "    # 评估BLEU分数\n",
    "    bleu_score = evaluate_bleu(model, dataset, '../dataset/dev_en.txt', '../dataset/dev_zh.txt', terminology = terminology,device = device)\n",
    "    print(f'BLEU-4 score: {bleu_score.score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 在测试集上进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-13T07:27:03.289563400Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(model: Seq2Seq, dataset: TranslationDataset, src_file: str, save_dir:str, terminology, device: torch.device):\n",
    "    model.eval()\n",
    "    src_sentences = load_sentences(src_file)\n",
    "    \n",
    "    translated_sentences = []\n",
    "    for src in src_sentences:\n",
    "        translated = translate_sentence(src, model, dataset, terminology, device)\n",
    "        #print(translated)\n",
    "        translated_sentences.append(translated)\n",
    "        #print(translated_sentences)\n",
    "\n",
    "    # 将列表元素连接成一个字符串，每个元素后换行\n",
    "    text = '\\n'.join(translated_sentences)\n",
    "\n",
    "    # 打开一个文件，如果不存在则创建，'w'表示写模式\n",
    "    with open(save_dir, 'w', encoding='utf-8') as f:\n",
    "        # 将字符串写入文件\n",
    "        f.write(text)\n",
    "\n",
    "    #return translated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-13T07:27:03.290563800Z"
    }
   },
   "outputs": [],
   "source": [
    "# 主函数\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 加载术语词典\n",
    "    terminology = load_terminology_dictionary('../dataset/en-zh.dic')\n",
    "    # 加载数据集和模型\n",
    "    dataset = TranslationDataset('../dataset/train.txt',terminology = terminology)\n",
    "\n",
    "    # 定义模型参数\n",
    "    INPUT_DIM = len(dataset.en_vocab)\n",
    "    OUTPUT_DIM = len(dataset.zh_vocab)\n",
    "    ENC_EMB_DIM = 256\n",
    "    DEC_EMB_DIM = 256\n",
    "    HID_DIM = 512\n",
    "    N_LAYERS = 2\n",
    "    ENC_DROPOUT = 0.5\n",
    "    DEC_DROPOUT = 0.5\n",
    "\n",
    "    # 初始化模型\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "    model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "    # 加载训练好的模型\n",
    "    model.load_state_dict(torch.load('./translation_model_GRU.pth'))\n",
    "    \n",
    "    save_dir = '../dataset/submit.txt'\n",
    "    inference(model, dataset, src_file=\"../dataset/test_en.txt\", save_dir = save_dir, terminology = terminology, device = device)\n",
    "    print(f\"翻译完成！文件已保存到{save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-13T07:27:03.291563400Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "base",
   "language": "python",
   "display_name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
